"""
# scikit-learn②

・回帰とは、ある値（目的変数と呼ばれます）を別の単一または複数の値（説明変数、機械学習では特に特徴量と呼ばれます）で説明するタスク

・線形回帰
    単回帰: 説明変数が1つ
    重回帰: 説明変数が2つ以上
・線形回帰における次元削減では、【説明変数】の数を削減する。
　⇒反対に、【目的変数】の削減をしない理由はモデルの精度を確保するため

・主成分分析は、分散・ばらつきが大きくなる方向を探して、元の次元と同じか、低い次元にデータを変換・圧縮する、次元削減の一手法
・主成分分析は、scikit-learnのdecompositonモジュールのPCAクラスを用いて実行することができる。

・機械学習を用いて構築した【分類モデル】の良し悪しを評価する指標に①適合率、②再現率、③F値、④正解率がある。これらは【混同行列】から計算する。
・一般的に①適合率と②再現率はトレードオフの関係にある。つまり、どちらか一方の指標を高くすると、もう一方の指標は低くなる。
　⇒このため、①適合率と②再現率の調和平均であるF値を調整して、両者をバランス良く使えるように調整する
・①適合率は曖昧なデータをNGにするイメージ
・②再現率は曖昧なデータをOKにするイメージ

・ランダムフォレストは、ブートストラップデータを用いて決定木を構築する処理を複数回繰り返し、各木の推定結果の多数決や平均値により分類・回帰を行う手法。
　アンサンブル学習の1つ。
　    ⇒ensembleモジュールのRandomForestClassifierクラス でランダムフォレストを実行可能

・決定木において、枝のことを【エッジ】ともいう
・決定木でデータを分割する時は、データの分割によってどれだけ得をするかについて考える。これを情報利得と呼ぶ。
　情報利得は親ノードの不純度から子ノードの不純度を差し引いたものとして定義される。

・サポートベクタマシンは、分類・回帰だけでなく外れ値検出にも使えるアルゴリズムであり、直線や平面などで分離できないデータであっても、高次元の空間に写して線形分離することにより分類を行うことを可能にする。
・サポートベクタマシンは、マージンを【最大】にすることにより決定境界を求めるが、これは、決定境界がサポートベクタから【遠くなり】多少のデータが変わっても誤った分類を行う可能性をを低くできる
　このようにして、未知のデータに対応できる汎化能力を持たせようとしている。
    ⇒svmモジュールのSVCクラス でサポートベクタマシンを実行可能

・交差検証とはデータセットを学習用とテスト用に分割する処理を繰り返し、モデルの構築と評価を複数回行う処理
・グリッドサーチを使うにはmodel_selectionモジュールのGridSearchCVクラスを利用する。
　これにより、交差検証における決定木の深さを求めることができる。
　※交差検証時の決定木の深さは実行毎に変動する。
・グリッドサーチのメソッドに引数「cv=10」のように渡すことで何分割するかを指定することができる
https://study.prime-strategy.co.jp/coverage/py3an1-39/

・機会学習の種類とアルゴリズムの一覧
https://study.prime-strategy.co.jp/wp-content/uploads/2021/02/%E7%AC%AC1%E5%9B%9E_Python3%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E6%A8%A1%E8%A9%A6_%E7%AC%AC40%E5%95%8F%E9%81%B8%E6%8A%9E%E8%82%A2%E2%91%A0%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-1536x797.png

・階層的クラスタリングには凝集型と分割型がある。
    ・凝集型は似ているデータをまとめていく
    ・分割型は似ていないデータを分割していく
    ・2つの型のうち、頻繁に利用されるのは凝集型
    ・凝集型はclusterモジュールのAgglomerativeClusteringクラスを用います。

    ・k-meansは最初にランダムにクラスタ中心を割り当て、クラスタ中心を各データとの距離を計算しながら修正し、最終的なクラスタ中心が収束するまで再計算を行いクラスタリングする手法
    ・k-meansは、scikit-learnのclusterモジュールのKMeansクラスを用いる
"""